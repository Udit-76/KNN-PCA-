{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "- K-Nearest Neighbors (KNN) is a supervised, instance-based algorithm. It stores the training data and, at prediction time:\n",
        "\n",
        "1. picks a value K\n",
        "\n",
        "2. computes distances from the query point to all training points (e.g., Euclidean, Manhattan)\n",
        "\n",
        "3. takes the K closest neighbors\n",
        "\n",
        "4. predicts:\n",
        "\n",
        "  - Classification: majority class among neighbors (optionally distance-weighted votes)\n",
        "\n",
        "  - Regression: average (or distance-weighted average) of neighbor targets\n",
        "\n",
        "  Lazy learner = no explicit model is fit; computation happens at query time."
      ],
      "metadata": {
        "id": "jceK2pjhUymA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "- As features (dimensions) grow:\n",
        "\n",
        "  - Data becomes sparse, and distances between points become less discriminative (nearest and farthest start to look similar).\n",
        "\n",
        "  - KNN’s distance comparisons become noisy → lower accuracy and higher variance.\n",
        "\n",
        "  Mitigations: scale features, remove irrelevant ones, and apply dimensionality reduction (e.g., PCA)."
      ],
      "metadata": {
        "id": "hlwyiYh8WX_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "- Principal Component Analysis (PCA) is an unsupervised linear dimensionality reduction that:\n",
        "\n",
        "  - Finds orthogonal directions (principal components) capturing maximal variance.\n",
        "\n",
        "  - Projects data onto these directions to reduce dimensions while preserving most variance.\n",
        "\n",
        "- PCA vs Feature Selection\n",
        "\n",
        "  - PCA (feature extraction): creates new features (linear combinations of originals).\n",
        "\n",
        "  - Feature selection: keeps a subset of original features (e.g., filter/wrapper methods)."
      ],
      "metadata": {
        "id": "WaKmOnXvWXzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important\n",
        "- Each principal component has an eigenvector (direction in feature space) and an eigenvalue (amount of variance captured along that direction).\n",
        "\n",
        "- Eigenvalue magnitude ⇒ importance of the component.\n",
        "\n",
        "- Sorting eigenvalues descending gives component order; explained variance ratio = eigenvalue / total variance."
      ],
      "metadata": {
        "id": "HQf3WmrkUycz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline\n",
        "- KNN relies on meaningful distances; high-dimensional, differently-scaled features can distort them.\n",
        "\n",
        "- Scaling → PCA → KNN often improves performance by:\n",
        "\n",
        "   - removing noise/redundancy,\n",
        "\n",
        "   - denoising via variance-focused projection,\n",
        "\n",
        "   - speeding up neighbor searches in fewer dimensions."
      ],
      "metadata": {
        "id": "LndY8qIpUySR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWqf8Xk1S05u",
        "outputId": "b243ec84-2d61-4d0a-88b1-dcce35b4da74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q6 Results:\n",
            "Accuracy (no scaling):   0.8056\n",
            "Accuracy (with scaling): 0.9722\n"
          ]
        }
      ],
      "source": [
        "# Q-6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# KNN without scaling\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "acc_no_scale = accuracy_score(y_test, knn_no_scale.predict(X_test))\n",
        "\n",
        "# KNN with scaling\n",
        "pipe_scaled = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "pipe_scaled.fit(X_train, y_train)\n",
        "acc_scaled = accuracy_score(y_test, pipe_scaled.predict(X_test))\n",
        "\n",
        "print(\"Q6 Results:\")\n",
        "print(f\"Accuracy (no scaling):   {acc_no_scale:.4f}\")\n",
        "print(f\"Accuracy (with scaling): {acc_scaled:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Scale features first\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# PCA (all components)\n",
        "pca_full = PCA()\n",
        "pca_full.fit(X_train_scaled)\n",
        "\n",
        "print(\"Q7 Results: PCA Explained Variance Ratios\")\n",
        "for i, ratio in enumerate(pca_full.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}, Cumulative: {pca_full.explained_variance_ratio_[:i+1].sum():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef6cPAanc9zB",
        "outputId": "d51d64c1-e9e7-43de-e668-5a5d8e6ddb3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q7 Results: PCA Explained Variance Ratios\n",
            "PC1: 0.3579, Cumulative: 0.3579\n",
            "PC2: 0.1927, Cumulative: 0.5506\n",
            "PC3: 0.1102, Cumulative: 0.6608\n",
            "PC4: 0.0727, Cumulative: 0.7335\n",
            "PC5: 0.0672, Cumulative: 0.8008\n",
            "PC6: 0.0513, Cumulative: 0.8521\n",
            "PC7: 0.0438, Cumulative: 0.8959\n",
            "PC8: 0.0250, Cumulative: 0.9209\n",
            "PC9: 0.0228, Cumulative: 0.9437\n",
            "PC10: 0.0188, Cumulative: 0.9624\n",
            "PC11: 0.0178, Cumulative: 0.9803\n",
            "PC12: 0.0126, Cumulative: 0.9928\n",
            "PC13: 0.0072, Cumulative: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "# PCA with top 2 components\n",
        "pca2 = PCA(n_components=2)\n",
        "X_train_pca2 = pca2.fit_transform(X_train_scaled)\n",
        "X_test_pca2 = pca2.transform(scaler.transform(X_test))\n",
        "\n",
        "# KNN on PCA data\n",
        "knn_pca2 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca2.fit(X_train_pca2, y_train)\n",
        "acc_pca2 = accuracy_score(y_test, knn_pca2.predict(X_test_pca2))\n",
        "\n",
        "print(\"Q8 Results:\")\n",
        "print(f\"Accuracy (original scaled data): {acc_scaled:.4f}\")\n",
        "print(f\"Accuracy (PCA top-2 comps):      {acc_pca2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFrIkJTrc9mR",
        "outputId": "d99e13e4-dfa3-4cc8-aec4-9826651de6ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q8 Results:\n",
            "Accuracy (original scaled data): 0.9722\n",
            "Accuracy (PCA top-2 comps):      0.9167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-9.Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "# Scale features\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Euclidean (p=2)\n",
        "knn_euclid = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2)\n",
        "knn_euclid.fit(X_train_scaled, y_train)\n",
        "acc_euclid = accuracy_score(y_test, knn_euclid.predict(X_test_scaled))\n",
        "\n",
        "# Manhattan (p=1)\n",
        "knn_manhat = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=1)\n",
        "knn_manhat.fit(X_train_scaled, y_train)\n",
        "acc_manhat = accuracy_score(y_test, knn_manhat.predict(X_test_scaled))\n",
        "\n",
        "print(\"Q9 Results:\")\n",
        "print(f\"Accuracy (Euclidean): {acc_euclid:.4f}\")\n",
        "print(f\"Accuracy (Manhattan): {acc_manhat:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSlOjkb0c9Zj",
        "outputId": "0b034108-95af-467a-e6a6-a12d0fb11af7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q9 Results:\n",
            "Accuracy (Euclidean): 0.9722\n",
            "Accuracy (Manhattan): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "- To handle a high-dimensional gene expression dataset with KNN, I would build the pipeline as follows:\n",
        "\n",
        "1. Preprocessing:\n",
        "First, I will standardize the gene features (zero mean, unit variance) so that distance calculations in KNN are fair.\n",
        "\n",
        "2. Dimensionality Reduction with PCA:\n",
        "Since the dataset has thousands of features but few samples, I will apply PCA to reduce dimensions.\n",
        "I will keep enough principal components to explain about 90–95% of the variance, or decide the number of PCs using cross-validation.\n",
        "\n",
        "3. KNN Classification:\n",
        "After PCA transformation, I will train a KNN classifier.\n",
        "I will tune the number of neighbors (K = 3, 5, 7, etc.) and also try different distance metrics like Euclidean and Manhattan.\n",
        "\n",
        "4. Evaluation:\n",
        "I will use cross-validation to check model accuracy and F1 score.\n",
        "I will also compare results for different numbers of components and K values.\n",
        "\n",
        "5. Justification:\n",
        "\n",
        "- PCA helps remove noise and reduces overfitting in high-dimensional data.\n",
        "\n",
        "- KNN is simple, interpretable, and works well after reducing dimensions.\n",
        "\n",
        "- Cross-validation makes sure results are reliable and not due to chance.\n",
        "\n",
        "- The pipeline is efficient and easy to update when new samples are added."
      ],
      "metadata": {
        "id": "s772w56LV3z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report # Import classification_report\n",
        "\n",
        "# Simulate high-dimensional dataset: 200 samples, 5000 features\n",
        "X_gene, y_gene = make_classification(\n",
        "    n_samples=200, n_features=5000, n_informative=50,\n",
        "    n_classes=3, random_state=42\n",
        ")\n",
        "\n",
        "# Split\n",
        "Xg_train, Xg_test, yg_train, yg_test = train_test_split(\n",
        "    X_gene, y_gene, test_size=0.2, stratify=y_gene, random_state=42\n",
        ")\n",
        "\n",
        "# Pipeline: scaling + PCA (retain 95% variance) + KNN\n",
        "pipe_gene = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=0.95, svd_solver=\"full\")),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2))\n",
        "])\n",
        "\n",
        "pipe_gene.fit(Xg_train, yg_train)\n",
        "yg_pred = pipe_gene.predict(Xg_test)\n",
        "\n",
        "print(\"Q10 Results:\")\n",
        "print(f\"Test Accuracy: {accuracy_score(yg_test, yg_pred):.4f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(yg_test, yg_pred))\n",
        "\n",
        "# Cross-validation for robustness\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(pipe_gene, X_gene, y_gene, cv=cv, scoring=\"accuracy\")\n",
        "print(f\"5-Fold CV Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPJVG9RqeZJr",
        "outputId": "a1211c06-22d8-4f3c-9968-52e39e8e722b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q10 Results:\n",
            "Test Accuracy: 0.3750\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.64      0.50        14\n",
            "           1       0.33      0.46      0.39        13\n",
            "           2       0.00      0.00      0.00        13\n",
            "\n",
            "    accuracy                           0.38        40\n",
            "   macro avg       0.25      0.37      0.30        40\n",
            "weighted avg       0.25      0.38      0.30        40\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-Fold CV Accuracy: 0.2950 ± 0.0400\n"
          ]
        }
      ]
    }
  ]
}